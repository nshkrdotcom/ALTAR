# ALTAR: Robust Design Specification

**Agent-Language Tool Abstraction Runtime**

A minimal, extensible protocol for cross-language tool execution with observability, cost tracking, and federation.

---

## Design Philosophy

### **Core Principles**

1. **Minimal Core, Maximal Extension**: Protocol defines interfaces, not implementations
2. **Separation of Concerns**: Transport ≠ Protocol ≠ Policy ≠ Application Logic
3. **No Magic Promotion Paths**: Explicit about distributed system complexities
4. **Python/Elixir Parity**: Same capabilities, different idioms
5. **Observable by Default**: Every tool call is traced, timed, and loggable

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                        ALTAR Stack                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                   │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │           Application Layer (User Code)                   │  │
│  │  - Business logic                                         │  │
│  │  - Policy decisions (cost limits, governance)            │  │
│  │  - Error handling                                         │  │
│  └────────────────┬─────────────────────────────────────────┘  │
│                   │                                              │
│  ┌────────────────┴─────────────────────────────────────────┐  │
│  │           ALTAR Client API                                │  │
│  │  - Tool.call/2                                            │  │
│  │  - Tool.stream/2                                          │  │
│  │  - Tool.batch/2                                           │  │
│  └────────────────┬─────────────────────────────────────────┘  │
│                   │                                              │
│  ┌────────────────┴─────────────────────────────────────────┐  │
│  │           Protocol Layer (ALTAR Core)                     │  │
│  │  - Request/Response format (MessagePack)                 │  │
│  │  - Schema validation (JSON Schema)                       │  │
│  │  - Error codes & semantics                               │  │
│  └────────────────┬─────────────────────────────────────────┘  │
│                   │                                              │
│  ┌────────────────┴─────────────────────────────────────────┐  │
│  │           Transport Layer (Pluggable)                     │  │
│  │  - Local: Function calls (same process)                  │  │
│  │  - IPC: Snakepit (Python ↔ Elixir)                       │  │
│  │  - Network: gRPC, HTTP/2 (distributed)                   │  │
│  └────────────────┬─────────────────────────────────────────┘  │
│                   │                                              │
│  ┌────────────────┴─────────────────────────────────────────┐  │
│  │           Tool Implementations                            │  │
│  │  - Python functions                                       │  │
│  │  - Elixir functions                                       │  │
│  │  - External APIs (OpenAI, Anthropic)                     │  │
│  │  - System commands (shell, dbt)                          │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                   │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │           Observability (Cross-Cutting)                   │  │
│  │  - Telemetry: Tool calls, latency, errors                │  │
│  │  - Tracing: OpenTelemetry spans                          │  │
│  │  - Metrics: Prometheus/StatsD                            │  │
│  │  - Cost Tracking: Token usage, API calls                 │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                   │
└─────────────────────────────────────────────────────────────────┘
```

---

## Core Protocol Specification

### **1. Request Format**

```elixir
# Elixir
%ALTAR.Request{
  # Required fields
  tool: String.t(),           # "python.ml.predict" | "elixir.db.query"
  method: String.t(),         # "execute" | "stream" | "batch"
  
  # Arguments (exactly one must be present)
  args: list() | nil,         # Positional: [1, 2, 3]
  kwargs: map() | nil,        # Named: %{x: 1, y: 2}
  
  # Metadata (all optional)
  request_id: String.t(),     # UUID for tracing
  timeout_ms: pos_integer(),  # Default: 30_000
  schema: map() | nil,        # JSON Schema for validation
  context: map()              # User-defined context (e.g., user_id, tenant_id)
}
```

```python
# Python (dataclass)
@dataclass
class AltarRequest:
    tool: str
    method: str = "execute"
    args: Optional[List[Any]] = None
    kwargs: Optional[Dict[str, Any]] = None
    request_id: str = field(default_factory=lambda: str(uuid4()))
    timeout_ms: int = 30000
    schema: Optional[Dict] = None
    context: Dict[str, Any] = field(default_factory=dict)
```

### **2. Response Format**

```elixir
%ALTAR.Response{
  # Status
  status: :ok | :error,
  
  # Result (only if status == :ok)
  result: term() | nil,
  
  # Error info (only if status == :error)
  error: %{
    code: String.t(),         # "TIMEOUT" | "VALIDATION_ERROR" | "TOOL_NOT_FOUND"
    message: String.t(),
    details: map() | nil
  } | nil,
  
  # Metadata (always present)
  metadata: %{
    request_id: String.t(),
    duration_ms: non_neg_integer(),
    tool: String.t(),
    timestamp: DateTime.t(),
    
    # Optional cost tracking
    cost: %{
      tokens: non_neg_integer() | nil,
      usd: float() | nil
    } | nil
  }
}
```

### **3. Standard Error Codes**

```elixir
defmodule ALTAR.ErrorCodes do
  # Client errors (4xx)
  @tool_not_found "TOOL_NOT_FOUND"           # Tool doesn't exist
  @validation_error "VALIDATION_ERROR"       # Args don't match schema
  @timeout "TIMEOUT"                         # Exceeded timeout_ms
  @unauthorized "UNAUTHORIZED"               # Authentication failed
  
  # Server errors (5xx)
  @execution_error "EXECUTION_ERROR"         # Tool raised exception
  @transport_error "TRANSPORT_ERROR"         # Network/IPC failure
  @internal_error "INTERNAL_ERROR"           # Unexpected error
  
  # Rate limiting
  @rate_limit_exceeded "RATE_LIMIT_EXCEEDED"
  @quota_exceeded "QUOTA_EXCEEDED"
end
```

---

## Transport Layers

### **Transport 1: Local (Same Process)**

**Use Case:** Elixir calling Elixir, Python calling Python

```elixir
defmodule ALTAR.Transport.Local do
  @behaviour ALTAR.Transport

  @impl true
  def call(request) do
    # Look up tool in local registry
    case ALTAR.Registry.lookup(request.tool) do
      {:ok, {module, function}} ->
        # Direct function call
        apply(module, function, [request.args || request.kwargs])
        
      :not_found ->
        {:error, %{code: "TOOL_NOT_FOUND"}}
    end
  end
end

# Usage
ALTAR.Tool.call("elixir.math.add", args: [1, 2], transport: :local)
# => {:ok, 3}
```

### **Transport 2: IPC via Snakepit (Cross-Language)**

**Use Case:** Elixir ↔ Python on same machine

```elixir
defmodule ALTAR.Transport.Snakepit do
  @behaviour ALTAR.Transport

  @impl true
  def call(request) do
    # Extract Python tool path
    # "python.ml.predict" -> pool: :ml, method: "predict"
    
    case parse_tool_path(request.tool) do
      {:ok, pool, method} ->
        Snakepit.call(pool, %{
          method: method,
          args: request.args,
          kwargs: request.kwargs,
          timeout: request.timeout_ms
        })
        
      {:error, reason} ->
        {:error, %{code: "TOOL_NOT_FOUND", message: reason}}
    end
  end
end

# Usage
ALTAR.Tool.call("python.ml.predict", 
  kwargs: %{model: "gpt2", input: "Hello"},
  transport: :snakepit
)
```

### **Transport 3: Network via gRPC (Distributed)**

**Use Case:** Services on different machines

```elixir
defmodule ALTAR.Transport.GRPC do
  @behaviour ALTAR.Transport

  @impl true
  def call(request) do
    # Connect to remote ALTAR service
    endpoint = discover_endpoint(request.tool)  # Service discovery
    
    # Send gRPC request
    GRPC.Stub.call(
      endpoint,
      ALTAR.ToolService.Execute,
      encode_request(request)
    )
  end
  
  defp discover_endpoint(tool) do
    # Consul, etcd, or DNS-based discovery
    # "python.ml.predict" -> "ml-service.prod.svc.cluster.local:50051"
  end
end
```

---

## Tool Registry & Discovery

### **Registry Design**

```elixir
defmodule ALTAR.Registry do
  @moduledoc """
  Tool registry with hierarchical namespacing.
  
  Tool paths use dot notation:
  - "python.ml.predict" -> Python ML service
  - "elixir.db.query" -> Elixir database function
  - "api.openai.chat" -> External API wrapper
  """

  # Tool metadata
  @type tool_spec :: %{
    path: String.t(),              # "python.ml.predict"
    transport: atom(),             # :local | :snakepit | :grpc
    schema: map() | nil,           # JSON Schema for args
    cost_model: map() | nil,       # Token counting, pricing
    metadata: map()                # Description, version, etc.
  }

  def register(tool_spec)
  def lookup(tool_path)
  def list(prefix \\ "")
end
```

### **Registration Examples**

```elixir
# Register Elixir tool
ALTAR.Registry.register(%{
  path: "elixir.db.users.fetch",
  transport: :local,
  handler: {MyApp.DB, :fetch_users},
  schema: %{
    "type" => "object",
    "properties" => %{
      "limit" => %{"type" => "integer", "default" => 100}
    }
  },
  metadata: %{
    description: "Fetch users from database",
    version: "1.0.0"
  }
})

# Register Python tool (via Snakepit)
ALTAR.Registry.register(%{
  path: "python.ml.predict",
  transport: :snakepit,
  pool: :ml_workers,
  method: "predict",
  schema: %{
    "type" => "object",
    "properties" => %{
      "model" => %{"type" => "string"},
      "input" => %{"type" => "string"}
    },
    "required" => ["model", "input"]
  },
  cost_model: %{
    tokens_per_request: 100,
    usd_per_1k_tokens: 0.002
  }
})

# Register external API
ALTAR.Registry.register(%{
  path: "api.openai.chat",
  transport: :http,
  endpoint: "https://api.openai.com/v1/chat/completions",
  auth: {:bearer, System.get_env("OPENAI_API_KEY")},
  cost_model: %{
    pricing: :dynamic  # Extract from response headers
  }
})
```

---

## Client API

### **Simple Execution**

```elixir
# Elixir
{:ok, result} = ALTAR.Tool.call("python.ml.predict",
  kwargs: %{model: "gpt2", input: "Hello world"}
)

# With timeout
{:ok, result} = ALTAR.Tool.call("python.ml.slow_task",
  args: [data],
  timeout_ms: 60_000
)

# With schema validation
{:ok, result} = ALTAR.Tool.call("elixir.db.query",
  kwargs: %{table: "users", limit: 10},
  validate: true  # Validates against registered schema
)
```

```python
# Python
from altar import Tool

result = Tool.call("elixir.db.query", 
    kwargs={"table": "users", "limit": 10}
)

# Async
result = await Tool.call_async("python.ml.predict",
    kwargs={"model": "gpt2", "input": "Hello"}
)
```

### **Streaming**

```elixir
# Stream results from long-running tool
ALTAR.Tool.stream("python.ml.train", 
  kwargs: %{model: "gpt2", epochs: 100},
  callback: fn event ->
    case event do
      {:progress, %{epoch: epoch, loss: loss}} ->
        IO.puts("Epoch #{epoch}: loss=#{loss}")
      
      {:complete, %{model: trained_model}} ->
        IO.puts("Training complete!")
      
      {:error, reason} ->
        IO.puts("Error: #{inspect(reason)}")
    end
  end
)
```

### **Batch Execution**

```elixir
# Execute multiple tool calls in parallel
requests = [
  %{tool: "python.ml.predict", kwargs: %{input: "text1"}},
  %{tool: "python.ml.predict", kwargs: %{input: "text2"}},
  %{tool: "python.ml.predict", kwargs: %{input: "text3"}}
]

{:ok, results} = ALTAR.Tool.batch(requests, max_concurrency: 10)

# Results maintain order
Enum.each(results, fn
  {:ok, prediction} -> IO.inspect(prediction)
  {:error, reason} -> IO.puts("Failed: #{inspect(reason)}")
end)
```

---

## Observability

### **Telemetry Events**

```elixir
# Emitted automatically on every tool call
:telemetry.execute(
  [:altar, :tool, :call, :start],
  %{system_time: System.system_time()},
  %{tool: tool_path, request_id: request_id}
)

:telemetry.execute(
  [:altar, :tool, :call, :stop],
  %{duration: duration_ms},
  %{tool: tool_path, request_id: request_id, status: :ok}
)

:telemetry.execute(
  [:altar, :tool, :call, :exception],
  %{duration: duration_ms},
  %{tool: tool_path, request_id: request_id, error_code: "TIMEOUT"}
)
```

### **OpenTelemetry Integration**

```elixir
defmodule ALTAR.Telemetry do
  require OpenTelemetry.Tracer, as: Tracer

  def trace_tool_call(request, fun) do
    Tracer.with_span "altar.tool.call", %{
      attributes: %{
        "altar.tool" => request.tool,
        "altar.method" => request.method,
        "altar.request_id" => request.request_id
      }
    } do
      case fun.() do
        {:ok, result} = success ->
          Tracer.set_attributes(%{"altar.status" => "ok"})
          success
        
        {:error, reason} = failure ->
          Tracer.set_status(:error, inspect(reason))
          Tracer.set_attributes(%{
            "altar.status" => "error",
            "altar.error_code" => reason.code
          })
          failure
      end
    end
  end
end
```

### **Cost Tracking**

```elixir
defmodule ALTAR.CostTracker do
  @moduledoc """
  Track cost per tool call.
  
  NOT enforced by ALTAR (application's responsibility),
  but ALTAR provides the data.
  """

  def record_cost(response) do
    if cost = response.metadata[:cost] do
      # Store in time-series DB
      :telemetry.execute(
        [:altar, :cost, :incurred],
        %{tokens: cost.tokens, usd: cost.usd},
        %{tool: response.metadata.tool, request_id: response.metadata.request_id}
      )
      
      # Application can query this data for budgeting
    end
  end
end

# Application-level budget enforcement
defmodule MyApp.BudgetPolicy do
  def check_budget(user_id) do
    spent_today = ALTAR.CostTracker.get_daily_spend(user_id)
    budget = get_user_budget(user_id)
    
    if spent_today < budget do
      :ok
    else
      {:error, :quota_exceeded}
    end
  end
end

# Usage in application
case MyApp.BudgetPolicy.check_budget(user_id) do
  :ok ->
    ALTAR.Tool.call("api.openai.chat", kwargs: %{prompt: prompt})
  
  {:error, :quota_exceeded} ->
    {:error, "Daily budget exceeded"}
end
```

---

## Federation (Multi-Cluster)

### **Problem Statement**

You have:
- **Cluster A**: Production (Elixir + Python workers)
- **Cluster B**: GPU cluster (Python ML workers)
- **Cluster C**: Data warehouse (Elixir analytics)

**Goal:** Call tools across clusters transparently

### **Design: Tool Path Routing**

```elixir
# Tool paths encode location
"cluster-a.python.ml.predict"  # Explicit cluster
"python.ml.predict"            # Use default routing rules

# Registry knows which cluster hosts which tools
ALTAR.Registry.register(%{
  path: "python.ml.predict",
  transport: :grpc,
  endpoint: "ml-service.cluster-b.example.com:50051",
  cluster: "cluster-b"
})

ALTAR.Registry.register(%{
  path: "elixir.analytics.report",
  transport: :grpc,
  endpoint: "analytics-service.cluster-c.example.com:50051",
  cluster: "cluster-c"
})

# Client code doesn't change
ALTAR.Tool.call("python.ml.predict", kwargs: %{...})
# ^ Automatically routes to cluster-b
```

### **Service Discovery Integration**

```elixir
defmodule ALTAR.Discovery.Consul do
  @behaviour ALTAR.Discovery

  @impl true
  def resolve_endpoint(tool_path) do
    # Query Consul for service hosting this tool
    service_name = tool_to_service(tool_path)
    
    case Consul.Health.service(service_name, passing: true) do
      {:ok, [service | _]} ->
        {:ok, "#{service.address}:#{service.port}"}
      
      {:ok, []} ->
        {:error, :no_healthy_instances}
    end
  end
  
  defp tool_to_service("python.ml." <> _), do: "ml-service"
  defp tool_to_service("elixir.db." <> _), do: "db-service"
  # ...
end
```

---

## Schema Validation

### **JSON Schema Integration**

```elixir
defmodule ALTAR.Validator do
  @moduledoc """
  Validate tool requests against JSON Schema.
  
  Uses ExJsonSchema or similar library.
  """

  def validate_request(request) do
    case ALTAR.Registry.lookup(request.tool) do
      {:ok, tool_spec} when not is_nil(tool_spec.schema) ->
        schema = tool_spec.schema
        args = request.args || request.kwargs
        
        case ExJsonSchema.Validator.validate(schema, args) do
          :ok ->
            :ok
          
          {:error, errors} ->
            {:error, %{
              code: "VALIDATION_ERROR",
              message: "Request does not match tool schema",
              details: %{errors: errors}
            }}
        end
      
      {:ok, _tool_spec} ->
        # No schema, skip validation
        :ok
      
      :not_found ->
        {:error, %{code: "TOOL_NOT_FOUND"}}
    end
  end
end

# Automatic validation (opt-in)
ALTAR.Tool.call("python.ml.predict",
  kwargs: %{model: "gpt2"},  # Missing required "input" field
  validate: true
)
# => {:error, %{code: "VALIDATION_ERROR", details: %{errors: ["Missing required field: input"]}}}
```

### **Schema Definition**

```elixir
# When registering a tool
ALTAR.Registry.register(%{
  path: "python.ml.predict",
  schema: %{
    "$schema" => "http://json-schema.org/draft-07/schema#",
    "type" => "object",
    "properties" => %{
      "model" => %{
        "type" => "string",
        "enum" => ["gpt2", "gpt3", "bert"]
      },
      "input" => %{
        "type" => "string",
        "minLength" => 1,
        "maxLength" => 4000
      },
      "temperature" => %{
        "type" => "number",
        "minimum" => 0.0,
        "maximum" => 2.0,
        "default" => 1.0
      }
    },
    "required" => ["model", "input"]
  }
})
```

---

## Python ↔ Elixir Parity

### **Python Client Library**

```python
# altar/client.py
import msgpack
from typing import Any, Dict, List, Optional

class Tool:
    @staticmethod
    def call(
        tool: str,
        args: Optional[List[Any]] = None,
        kwargs: Optional[Dict[str, Any]] = None,
        timeout_ms: int = 30000,
        validate: bool = False
    ) -> Any:
        """
        Call a tool via ALTAR protocol.
        
        Examples:
            # Call Elixir function
            result = Tool.call("elixir.db.query", kwargs={"table": "users"})
            
            # Call Python function (local)
            result = Tool.call("python.ml.predict", kwargs={"input": "text"})
        """
        request = AltarRequest(
            tool=tool,
            args=args,
            kwargs=kwargs,
            timeout_ms=timeout_ms
        )
        
        if validate:
            # Validate against schema
            _validate_request(request)
        
        # Send via transport
        transport = _get_transport(tool)
        response = transport.send(request)
        
        if response.status == "ok":
            return response.result
        else:
            raise AltarError(response.error)
    
    @staticmethod
    async def call_async(tool: str, **kwargs) -> Any:
        """Async version of call()"""
        pass
    
    @staticmethod
    def stream(tool: str, callback, **kwargs):
        """Stream results from tool"""
        pass
    
    @staticmethod
    def batch(requests: List[Dict], max_concurrency: int = 10) -> List[Any]:
        """Execute multiple tool calls in parallel"""
        pass
```

### **Python Tool Server**

```python
# altar/server.py
from altar import Registry, Transport

# Register Python tools
@Registry.register("python.ml.predict")
def predict(model: str, input: str, temperature: float = 1.0) -> str:
    """ML prediction tool."""
    model = load_model(model)
    return model.predict(input, temperature=temperature)

# Start ALTAR server (listens for requests from Elixir)
if __name__ == "__main__":
    server = Transport.GRPC.Server(port=50051)
    server.serve()
```

---

## Example Use Cases

### **Use Case 1: Elixir Phoenix → Python ML**

```elixir
defmodule MyAppWeb.ChatController do
  use MyAppWeb, :controller

  def chat(conn, %{"message" => message}) do
    # Call Python ML service via ALTAR
    {:ok, response} = ALTAR.Tool.call("python.ml.chat",
      kwargs: %{
        model: "gpt4",
        messages: [%{role: "user", content: message}]
      },
      timeout_ms: 60_000
    )

    json(conn, %{response: response.content})
  end
end
```

### **Use Case 2: Python Training Script → Elixir DB**

```python
# train.py
from altar import Tool

# Fetch training data from Elixir database
training_data = Tool.call("elixir.db.training_data.fetch",
    kwargs={"dataset": "user_feedback", "limit": 10000}
)

# Train model
model = train_model(training_data)

# Save model metadata to Elixir DB
Tool.call("elixir.db.models.save",
    kwargs={"name": "gpt2_v2", "path": "s3://models/gpt2_v2.pt"}
)
```

### **Use Case 3: Distributed Pipeline**

```elixir
defmodule MyApp.Pipeline do
  def run_pipeline(data) do
    # Step 1: Preprocess in Python
    {:ok, preprocessed} = ALTAR.Tool.call("python.preprocess", args: [data])
    
    # Step 2: Analyze in Elixir
    {:ok, analysis} = ALTAR.Tool.call("elixir.analytics.analyze", args: [preprocessed])
    
    # Step 3: Generate report in Python
    {:ok, report} = ALTAR.Tool.call("python.reports.generate", args: [analysis])
    
    {:ok, report}
  end
end
```

---

## Implementation Roadmap

### **Phase 1: Core Protocol (2 weeks)**

- [ ] Define `ALTAR.Request` and `ALTAR.Response` structs
- [ ] Implement `ALTAR.Registry` (in-memory)
- [ ] Implement `ALTAR.Transport.Local` (Elixir → Elixir)
- [ ] Implement `ALTAR.Validator` (JSON Schema)
- [ ] Write core tests (95%+ coverage)

### **Phase 2: Snakepit Integration (2 weeks)**

- [ ] Implement `ALTAR.Transport.Snakepit` (Elixir → Python)
- [ ] Python client library (`altar` package)
- [ ] Python server (receives calls from Elixir)
- [ ] Bidirectional: Python → Elixir via ALTAR
- [ ] Integration tests

### **Phase 3: Observability (1 week)**

- [ ] Telemetry events
- [ ] OpenTelemetry integration
- [ ] Cost tracking
- [ ] Metrics (Prometheus)

### **Phase 4: Federation (2 weeks)**

- [ ] Implement `ALTAR.Transport.GRPC`
- [ ] Service discovery (Consul integration)
- [ ] Multi-cluster routing
- [ ] End-to-end distributed tests

### **Phase 5: Production Features (2 weeks)**

- [ ] Circuit breaker per tool
- [ ] Retry policies
- [ ] Rate limiting
- [ ] API documentation (ExDoc)
- [ ] Python package (PyPI)

---

## Key Design Decisions

### **1. Why MessagePack over JSON?**

- **Performance**: 2-5x faster serialization
- **Binary support**: Encode binary data without base64
- **Compact**: Smaller payloads (~30% size reduction)
- **Schema evolution**: Add fields without breaking old clients

### **2. Why Not Just Use gRPC Everywhere?**

- **gRPC is a transport, not a protocol**: ALTAR sits above transport
- **Flexibility**: Can swap transports (gRPC, HTTP/2, Snakepit, local) without changing tool code
- **Local optimization**: Function calls are faster than gRPC for same-process tools

### **3. Why JSON Schema for Validation?**

- **Standard**: Widely adopted, good tooling
- **Language-agnostic**: Works for both Python and Elixir
- **Expressive**: Supports complex validation rules
- **Alternative**: Could use Ecto schemas (Elixir-only) or Pydantic (Python-only)

### **4. Why Not Build Cost Manager Into ALTAR?**

**Cost management is application policy, not protocol concern.**

- Different apps have different budgets
- Different apps have different cost models
- ALTAR provides cost data, application decides what to do with it

**Better separation:**
- ALTAR: "This call cost 1000 tokens ($0.02)"
- Application: "User has exceeded budget" → Reject call

### **5. Why No "Promotion Path" (LATER → GRID)?**

**Distributed systems are fundamentally different:**

- Network can fail (local can't)
- Latency is 1000x higher
- Serialization required
- Different error modes

**Instead:** Explicit tool paths
- `"local.ml.predict"` → Local function
- `"python.ml.predict"` → Via Snakepit (same machine)
- `"cluster-b.ml.predict"` → Via gRPC (different machine)

**Application decides** based on requirements, not ALTAR.

---

## What ALTAR Is NOT

### ❌ **NOT an RPC Framework**

ALTAR uses RPC (gRPC, Snakepit), but adds:
- Tool registry & discovery
- Schema validation
- Cost tracking
- Observability

### ❌ **NOT a Workflow Engine**

ALTAR doesn't orchestrate multi-step workflows. Use:
- **FlowStone** for DAGs
- **Oban** for job queues
- **GenStage** for streaming

ALTAR is the **"function call"** primitive used by workflow engines.

### ❌ **NOT an API Gateway**

ALTAR is library-first (import and use), not proxy-first.

For API gateway, wrap ALTAR tools:
```elixir
# Phoenix endpoint
post "/tools/:tool_path" do
  ALTAR.Tool.call(tool_path, kwargs: conn.body_params)
end
```

---

## Summary

**ALTAR is:**

1. **Minimal protocol** for cross-language tool calls
2. **Transport-agnostic** (local, Snakepit, gRPC, HTTP)
3. **Observable by default** (telemetry, tracing, cost tracking)
4. **Schema-validated** (JSON Schema)
5. **Language-neutral** (Elixir ↔ Python parity)

**ALTAR is NOT:**

1. ❌ Cost manager (application's job)
2. ❌ Governance engine (application's job)
3. ❌ Workflow orchestrator (use FlowStone)
4. ❌ Magic promotion path (be explicit about distributed)

**Implementation strategy:**

- Start with **Phase 1** (core protocol) using this spec
- Integrate with **Snakepit** in Phase 2
- Add observability and federation in Phases 3-4
- Keep it **minimal** and **extensible**

**This design removes all overengineering while keeping ALTAR useful and production-ready.**

Would you like me to:
1. Generate Elixir code for Phase 1 (core protocol)?
2. Design the Python client library API in detail?
3. Specify the gRPC `.proto` file for distributed transport?
